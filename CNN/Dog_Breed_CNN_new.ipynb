{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN from Sctrach for Stanford Dog Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing liberaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization, Flatten, Conv2D\n",
    "from tensorflow.keras.layers import MaxPool2D, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import Sequential\n",
    "import os,sys\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loading Data using Keras ImageDatagenerator <br/>\n",
    "Params : <br/>\n",
    "rescale all images and divide by 255 for color<br/>\n",
    "Shear mapping or shear range set to 0.2<br/>\n",
    "zoom image with 20% or 0.2<br/>\n",
    "horizontal flip image set to true<br/>\n",
    "\n",
    "above params to create image generator for training<br/>\n",
    "\n",
    "2. Using flow_from_directory to create image dataset from created ImageDatagenerator<br/>\n",
    "Params : <br/>\n",
    "a. training folder (annotated image after applying crop according to the annotation from dataset ref. Prepping_Data)<br/>\n",
    "b. target image size (I have used (128X128), can be chhanged to 256X256 or even smaller (64X64), check with what size image details is not diminished )<br/>\n",
    "c. batch size = 32<br/>\n",
    "d. class_mode = 'categorical' if output is of more than two output in case of two output - 'binary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12000 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        'anotated/train',\n",
    "        target_size=(128, 128),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as training Data but some changes as we are not doing any transformation on test dataset<br/>\n",
    "1. For ImageDataGenerator only use rescale of colors<br/>\n",
    "\n",
    "2. For Dataset generation use same as training data <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8580 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        'anotated/test',\n",
    "        target_size=(128, 128),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initilizing CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add first CNN layer <br/>\n",
    "Using 3X3 Convolution activation relu and input shape is of 128X128X3 (for rgb) <br/>\n",
    "Using MaxPool2d<br/>\n",
    "Introdcing noise/regularization using dropout layer with 20% params drop<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu',input_shape=[128, 128, 3]))\n",
    "cnn.add(MaxPool2D(pool_size=2, strides=2))\n",
    "cnn.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Second Layer of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add( Conv2D(filters=64, kernel_size=3, activation='relu'))\n",
    "cnn.add(MaxPool2D(pool_size=2, strides=2))\n",
    "cnn.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Third Layer of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Conv2D(filters=128, kernel_size=3, activation='relu'))\n",
    "cnn.add(MaxPool2D(pool_size=2, strides=2))\n",
    "cnn.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Fourth Layer of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Conv2D(filters=256, kernel_size=3, activation='relu'))\n",
    "cnn.add(MaxPool2D(pool_size=2, strides=2))\n",
    "cnn.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding GlobalAveragePooling on final params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(GlobalAveragePooling2D())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Dense layer with activation function of 'softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Dense(units=120, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Summary of CNN layer <br/>\n",
    "Total params: 419,256<br/>\n",
    "Trainable params: 419,256<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 126, 126, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 63, 63, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 63, 63, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 61, 61, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 12, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               30840     \n",
      "=================================================================\n",
      "Total params: 419,256\n",
      "Trainable params: 419,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling CNN\n",
    "Optimizer Used : Stochastic Gradient Descent (Adam in tf)<br/>\n",
    "loss function : for categorical feature training use 'categorical_crossentropy' <br/>\n",
    "metrics : 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting Data in Compiled CNN <br/>\n",
    "validation data = test_set <br/>\n",
    "epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "375/375 [==============================] - 790s 2s/step - loss: 4.6757 - accuracy: 0.0133 - val_loss: 4.4649 - val_accuracy: 0.0291\n",
      "Epoch 2/25\n",
      "375/375 [==============================] - 297s 793ms/step - loss: 4.4319 - accuracy: 0.0268 - val_loss: 4.3938 - val_accuracy: 0.0364\n",
      "Epoch 3/25\n",
      "375/375 [==============================] - 98s 261ms/step - loss: 4.3293 - accuracy: 0.0391 - val_loss: 4.2204 - val_accuracy: 0.0540\n",
      "Epoch 4/25\n",
      "375/375 [==============================] - 99s 264ms/step - loss: 4.1579 - accuracy: 0.0584 - val_loss: 3.9971 - val_accuracy: 0.0774\n",
      "Epoch 5/25\n",
      "375/375 [==============================] - 98s 261ms/step - loss: 3.9904 - accuracy: 0.0796 - val_loss: 3.9222 - val_accuracy: 0.0955\n",
      "Epoch 6/25\n",
      "375/375 [==============================] - 99s 264ms/step - loss: 3.8518 - accuracy: 0.1001 - val_loss: 3.8732 - val_accuracy: 0.0967\n",
      "Epoch 7/25\n",
      "375/375 [==============================] - 102s 273ms/step - loss: 3.7334 - accuracy: 0.1129 - val_loss: 3.6769 - val_accuracy: 0.1282\n",
      "Epoch 8/25\n",
      "375/375 [==============================] - 79s 209ms/step - loss: 3.6157 - accuracy: 0.1374 - val_loss: 3.7038 - val_accuracy: 0.1206\n",
      "Epoch 9/25\n",
      "375/375 [==============================] - 67s 179ms/step - loss: 3.5189 - accuracy: 0.1468 - val_loss: 3.5118 - val_accuracy: 0.1577\n",
      "Epoch 10/25\n",
      "375/375 [==============================] - 64s 171ms/step - loss: 3.4232 - accuracy: 0.1706 - val_loss: 3.3858 - val_accuracy: 0.1714\n",
      "Epoch 11/25\n",
      "375/375 [==============================] - 68s 181ms/step - loss: 3.3361 - accuracy: 0.1857 - val_loss: 3.3414 - val_accuracy: 0.1876\n",
      "Epoch 12/25\n",
      "375/375 [==============================] - 67s 178ms/step - loss: 3.2394 - accuracy: 0.2026 - val_loss: 3.3981 - val_accuracy: 0.1740\n",
      "Epoch 13/25\n",
      "375/375 [==============================] - 68s 182ms/step - loss: 3.1577 - accuracy: 0.2183 - val_loss: 3.2914 - val_accuracy: 0.1900\n",
      "Epoch 14/25\n",
      "375/375 [==============================] - 88s 235ms/step - loss: 3.0953 - accuracy: 0.2271 - val_loss: 3.3226 - val_accuracy: 0.1910\n",
      "Epoch 15/25\n",
      "375/375 [==============================] - 117s 313ms/step - loss: 3.0186 - accuracy: 0.2453 - val_loss: 3.2279 - val_accuracy: 0.2078\n",
      "Epoch 16/25\n",
      "375/375 [==============================] - 105s 280ms/step - loss: 2.9790 - accuracy: 0.2503 - val_loss: 3.2053 - val_accuracy: 0.2147\n",
      "Epoch 17/25\n",
      "375/375 [==============================] - 101s 268ms/step - loss: 2.9028 - accuracy: 0.2717 - val_loss: 3.1701 - val_accuracy: 0.2219\n",
      "Epoch 18/25\n",
      "375/375 [==============================] - 101s 269ms/step - loss: 2.8444 - accuracy: 0.2791 - val_loss: 3.0977 - val_accuracy: 0.2330\n",
      "Epoch 19/25\n",
      "375/375 [==============================] - 97s 258ms/step - loss: 2.7962 - accuracy: 0.2865 - val_loss: 3.2406 - val_accuracy: 0.2132\n",
      "Epoch 20/25\n",
      "375/375 [==============================] - 89s 238ms/step - loss: 2.7218 - accuracy: 0.3059 - val_loss: 3.0929 - val_accuracy: 0.2430\n",
      "Epoch 21/25\n",
      "375/375 [==============================] - 100s 267ms/step - loss: 2.7001 - accuracy: 0.3100 - val_loss: 3.1737 - val_accuracy: 0.2282\n",
      "Epoch 22/25\n",
      "375/375 [==============================] - 74s 198ms/step - loss: 2.6352 - accuracy: 0.3218 - val_loss: 3.1750 - val_accuracy: 0.2404\n",
      "Epoch 23/25\n",
      "375/375 [==============================] - 71s 188ms/step - loss: 2.5916 - accuracy: 0.3320 - val_loss: 3.1501 - val_accuracy: 0.2456\n",
      "Epoch 24/25\n",
      "375/375 [==============================] - 65s 174ms/step - loss: 2.5496 - accuracy: 0.3382 - val_loss: 2.8870 - val_accuracy: 0.2793\n",
      "Epoch 25/25\n",
      "375/375 [==============================] - 65s 173ms/step - loss: 2.5072 - accuracy: 0.3517 - val_loss: 3.0804 - val_accuracy: 0.2503\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/device:GPU:0\"):\n",
    "    cnn.fit(training_set, validation_data = test_set, epochs = 25 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting Data in Compiled CNN <br/>\n",
    "validation data = test_set <br/>\n",
    "epochs = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "375/375 [==============================] - 72s 193ms/step - loss: 2.4905 - accuracy: 0.3542 - val_loss: 2.9663 - val_accuracy: 0.2740\n",
      "Epoch 2/35\n",
      "375/375 [==============================] - 82s 218ms/step - loss: 2.4414 - accuracy: 0.3619 - val_loss: 2.9988 - val_accuracy: 0.2650\n",
      "Epoch 3/35\n",
      "375/375 [==============================] - 96s 255ms/step - loss: 2.4029 - accuracy: 0.3730 - val_loss: 3.0119 - val_accuracy: 0.2608\n",
      "Epoch 4/35\n",
      "375/375 [==============================] - 93s 247ms/step - loss: 2.3668 - accuracy: 0.3750 - val_loss: 2.9046 - val_accuracy: 0.2822\n",
      "Epoch 5/35\n",
      "375/375 [==============================] - 92s 245ms/step - loss: 2.3466 - accuracy: 0.3837 - val_loss: 3.0328 - val_accuracy: 0.2723\n",
      "Epoch 6/35\n",
      "375/375 [==============================] - 494s 1s/step - loss: 2.2923 - accuracy: 0.3997 - val_loss: 2.9855 - val_accuracy: 0.2767\n",
      "Epoch 7/35\n",
      "375/375 [==============================] - 154s 410ms/step - loss: 2.2871 - accuracy: 0.3930 - val_loss: 2.9043 - val_accuracy: 0.2900\n",
      "Epoch 8/35\n",
      "375/375 [==============================] - 67s 179ms/step - loss: 2.2476 - accuracy: 0.4021 - val_loss: 3.1415 - val_accuracy: 0.2608\n",
      "Epoch 9/35\n",
      "375/375 [==============================] - 67s 178ms/step - loss: 2.2588 - accuracy: 0.3980 - val_loss: 2.8386 - val_accuracy: 0.3003\n",
      "Epoch 10/35\n",
      "375/375 [==============================] - 64s 171ms/step - loss: 2.2045 - accuracy: 0.4090 - val_loss: 2.8502 - val_accuracy: 0.3047\n",
      "Epoch 11/35\n",
      "375/375 [==============================] - 63s 169ms/step - loss: 2.1858 - accuracy: 0.4125 - val_loss: 2.9842 - val_accuracy: 0.2826\n",
      "Epoch 12/35\n",
      "375/375 [==============================] - 69s 183ms/step - loss: 2.1577 - accuracy: 0.4198 - val_loss: 3.0917 - val_accuracy: 0.2663\n",
      "Epoch 13/35\n",
      "375/375 [==============================] - 84s 225ms/step - loss: 2.1330 - accuracy: 0.4266 - val_loss: 2.9173 - val_accuracy: 0.2949\n",
      "Epoch 14/35\n",
      "375/375 [==============================] - 74s 197ms/step - loss: 2.1156 - accuracy: 0.4339 - val_loss: 3.0814 - val_accuracy: 0.2738\n",
      "Epoch 15/35\n",
      "375/375 [==============================] - 74s 198ms/step - loss: 2.0613 - accuracy: 0.4441 - val_loss: 2.9275 - val_accuracy: 0.2970\n",
      "Epoch 16/35\n",
      "375/375 [==============================] - 75s 201ms/step - loss: 2.0632 - accuracy: 0.4433 - val_loss: 3.0381 - val_accuracy: 0.2788\n",
      "Epoch 17/35\n",
      "375/375 [==============================] - 70s 186ms/step - loss: 2.0441 - accuracy: 0.4474 - val_loss: 2.9290 - val_accuracy: 0.3016\n",
      "Epoch 18/35\n",
      "375/375 [==============================] - 75s 199ms/step - loss: 2.0257 - accuracy: 0.4518 - val_loss: 2.8796 - val_accuracy: 0.3124\n",
      "Epoch 19/35\n",
      "375/375 [==============================] - 68s 182ms/step - loss: 1.9990 - accuracy: 0.4567 - val_loss: 2.8588 - val_accuracy: 0.3181\n",
      "Epoch 20/35\n",
      "375/375 [==============================] - 72s 191ms/step - loss: 1.9954 - accuracy: 0.4543 - val_loss: 3.0228 - val_accuracy: 0.2843\n",
      "Epoch 21/35\n",
      "375/375 [==============================] - 67s 180ms/step - loss: 1.9945 - accuracy: 0.4558 - val_loss: 3.0726 - val_accuracy: 0.2855\n",
      "Epoch 22/35\n",
      "375/375 [==============================] - 70s 187ms/step - loss: 1.9494 - accuracy: 0.4679 - val_loss: 2.9532 - val_accuracy: 0.2966\n",
      "Epoch 23/35\n",
      "375/375 [==============================] - 71s 190ms/step - loss: 1.9520 - accuracy: 0.4636 - val_loss: 2.8442 - val_accuracy: 0.3126\n",
      "Epoch 24/35\n",
      "375/375 [==============================] - 76s 203ms/step - loss: 1.9132 - accuracy: 0.4787 - val_loss: 2.9266 - val_accuracy: 0.3125\n",
      "Epoch 25/35\n",
      "375/375 [==============================] - 75s 201ms/step - loss: 1.9255 - accuracy: 0.4692 - val_loss: 2.8836 - val_accuracy: 0.3183\n",
      "Epoch 26/35\n",
      "375/375 [==============================] - 75s 200ms/step - loss: 1.8899 - accuracy: 0.4795 - val_loss: 2.8771 - val_accuracy: 0.3146\n",
      "Epoch 27/35\n",
      "375/375 [==============================] - 69s 184ms/step - loss: 1.8857 - accuracy: 0.4782 - val_loss: 2.9167 - val_accuracy: 0.3138\n",
      "Epoch 28/35\n",
      "375/375 [==============================] - 75s 201ms/step - loss: 1.8648 - accuracy: 0.4884 - val_loss: 2.7891 - val_accuracy: 0.3367\n",
      "Epoch 29/35\n",
      "375/375 [==============================] - 71s 190ms/step - loss: 1.8498 - accuracy: 0.4873 - val_loss: 3.0300 - val_accuracy: 0.3059\n",
      "Epoch 30/35\n",
      "375/375 [==============================] - 69s 184ms/step - loss: 1.8425 - accuracy: 0.4936 - val_loss: 2.9088 - val_accuracy: 0.3155\n",
      "Epoch 31/35\n",
      "375/375 [==============================] - 70s 187ms/step - loss: 1.8277 - accuracy: 0.4983 - val_loss: 2.8624 - val_accuracy: 0.3239\n",
      "Epoch 32/35\n",
      "375/375 [==============================] - 72s 191ms/step - loss: 1.8249 - accuracy: 0.4895 - val_loss: 2.8884 - val_accuracy: 0.3146\n",
      "Epoch 33/35\n",
      "375/375 [==============================] - 67s 180ms/step - loss: 1.7926 - accuracy: 0.5022 - val_loss: 3.0797 - val_accuracy: 0.2993\n",
      "Epoch 34/35\n",
      "375/375 [==============================] - 71s 190ms/step - loss: 1.7890 - accuracy: 0.5030 - val_loss: 2.9312 - val_accuracy: 0.3111\n",
      "Epoch 35/35\n",
      "375/375 [==============================] - 72s 192ms/step - loss: 1.7838 - accuracy: 0.5076 - val_loss: 2.8887 - val_accuracy: 0.3207\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/device:GPU:0\"):\n",
    "    cnn.fit(training_set, validation_data = test_set, epochs = 35 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN are trained on training Data and Validated on test data<br/>\n",
    "<br/>\n",
    "for epochs : 25 <br/>\n",
    "loss: 2.5072 - accuracy: 0.3517 - val_loss: 3.0804 - val_accuracy: 0.2503 <br/>\n",
    "<br/>\n",
    "<br/>\n",
    "for epochs : 35 <br/>\n",
    "loss: 1.7838 - accuracy: 0.5076 - val_loss: 2.8887 - val_accuracy: 0.3207"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
